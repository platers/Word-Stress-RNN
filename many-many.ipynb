{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import cmudict\n",
    "#http://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "ARPAbet = ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "arpa = {}\n",
    "for i in range(len(ARPAbet)):\n",
    "    arpa[ARPAbet[i]] = i\n",
    "print len(ARPAbet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = cmudict.dict()\n",
    "e = cmudict.entries()\n",
    "x_train = []\n",
    "y_train = []\n",
    "for x in e:\n",
    "    #print x\n",
    "    w = x[0]\n",
    "    skip = False\n",
    "    for c in w:\n",
    "        if ord(c) < ord('a') or ord(c) > ord('z'):\n",
    "            skip = True\n",
    "    if skip:\n",
    "        continue\n",
    "    w.encode('ascii','ignore')\n",
    "    w = list(w)\n",
    "    x_train.append(w)\n",
    "    p = []\n",
    "    for y in x[1]:\n",
    "        stress = -1\n",
    "        y.encode('ascii','ignore')\n",
    "        if(y[0:-1] not in arpa):\n",
    "            continue\n",
    "        stress = arpa[y[0:-1]] * 2\n",
    "        if(y[len(y) - 1] == '1'):\n",
    "            stress += 1\n",
    "        p.append(stress)\n",
    "    y_train.append(p)\n",
    "maxlen = 35\n",
    "X = np.zeros((len(x_train), maxlen, 26), dtype=np.bool)\n",
    "y = np.zeros((len(y_train), maxlen, len(ARPAbet) * 2 + 1), dtype=np.bool)\n",
    "for i, word in enumerate(x_train):\n",
    "    for t, char in enumerate(word):\n",
    "        #print word\n",
    "        X[i, t, ord(char) - ord('a')] = 1\n",
    "    \n",
    "    for j, s in enumerate(y_train[i]):\n",
    "        y[i, j, s] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'G', u'UH2', u'D', u'B', u'AY1']]\n",
      "(u'abducted', [u'AH0', u'B', u'D', u'AH1', u'K', u'T', u'IH0', u'D'])\n",
      "[u'b', u'l', u'a', u'c', u'k', u'o', u'u', u't', u's']\n",
      "33\n",
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print d[\"goodbye\"]\n",
    "print e[110]\n",
    "print x_train[11101]\n",
    "c = 0\n",
    "for x in e:\n",
    "    c = max(c, len(x[0]))\n",
    "print c\n",
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_10 (LSTM)                     (None, 300)         392400      lstm_input_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_4 (RepeatVector)      (None, 35, 300)     0           lstm_10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                     (None, 35, 100)     160400      repeatvector_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "timedistributeddense_3 (TimeDistrib(None, 35, 78)      7878        lstm_11[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 560678\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import TimeDistributedDense, Dropout, Activation, Dense, RepeatVector\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import RMSprop, adam\n",
    "model = Sequential()\n",
    "model.add(LSTM(300, input_shape=(maxlen, 26)))\n",
    "model.add(RepeatVector(35))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(TimeDistributedDense(len(ARPAbet) * 2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam(), metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "124996/124996 [==============================] - 287s - loss: 0.2180 - acc: 0.0145   \n",
      "Epoch 2/15\n",
      "124996/124996 [==============================] - 298s - loss: 0.2163 - acc: 0.0146   \n",
      "Epoch 3/15\n",
      "124996/124996 [==============================] - 297s - loss: 0.2163 - acc: 0.0145   \n",
      "Epoch 4/15\n",
      "124996/124996 [==============================] - 287s - loss: 0.2162 - acc: 0.0145   \n",
      "Epoch 5/15\n",
      "124996/124996 [==============================] - 286s - loss: 0.2162 - acc: 0.0145   \n",
      "Epoch 6/15\n",
      "124996/124996 [==============================] - 285s - loss: 0.2162 - acc: 0.0146   \n",
      "Epoch 7/15\n",
      "124996/124996 [==============================] - 292s - loss: 0.2162 - acc: 0.0145   \n",
      "Epoch 8/15\n",
      "124996/124996 [==============================] - 291s - loss: 0.2161 - acc: 0.0145   \n",
      "Epoch 9/15\n",
      "124996/124996 [==============================] - 279s - loss: 0.2161 - acc: 0.0146   \n",
      "Epoch 10/15\n",
      "124996/124996 [==============================] - 279s - loss: 0.2161 - acc: 0.0145   \n",
      "Epoch 11/15\n",
      "124996/124996 [==============================] - 279s - loss: 0.2161 - acc: 0.0145   \n",
      "Epoch 12/15\n",
      "124996/124996 [==============================] - 279s - loss: 0.2161 - acc: 0.0145   \n",
      "Epoch 13/15\n",
      "124996/124996 [==============================] - 299s - loss: 0.2161 - acc: 0.0146   \n",
      "Epoch 14/15\n",
      "124996/124996 [==============================] - 348s - loss: 0.2161 - acc: 0.0145   \n",
      "Epoch 15/15\n",
      "124996/124996 [==============================] - 324s - loss: 0.2161 - acc: 0.0145   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model.load_weights('model.h5')\n",
    "model.fit(X, y, batch_size=100, nb_epoch=15)\n",
    "model.save_weights('many.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 35L, 78L)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stress(w):\n",
    "    w.encode('ascii','ignore')\n",
    "    w = list(w)\n",
    "    X = np.zeros((1, maxlen, 26), dtype=np.bool)\n",
    "    for t, char in enumerate(w):\n",
    "        X[0, t, ord(char) - ord('a')] = 1\n",
    "    m = model.predict(X)\n",
    "    #print X\n",
    "    #a = np.argmax(m)\n",
    "    #print a\n",
    "    for t, c in enumerate(m[0]):\n",
    "        a = np.argmax(c)\n",
    "        if(a == 0)\n",
    "    return m.shape\n",
    "    return(ARPAbet[a])\n",
    "print stress('l')\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('finalweights.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
